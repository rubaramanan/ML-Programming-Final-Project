{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Hierarchical Attentional Hybrid NeuralNetworks and XLNet for document classification\n",
    "\n",
    "#### Group Member Names : \n",
    "Ramanan Tayalan (200565744)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM : \n",
    "To comparatively analyse the feasibility of the proposed Model in the paper \"Hierarchical Attentional Hybrid Neural Networks for Document Classification\" released in 2019 with XL net model in IMDB dataset.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo: \n",
    "https://github.com/luisfredgs/cnn-hierarchical-network-for-document-classification\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER: # have to write\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "* Try to replicate the results given in paper on document classification datasets with HAHNN model.\n",
    "* choose imdb datset as standard dataset.\n",
    "* To compare the performance of HAHNN model, trained XLNet model on the above-mentioned dataset.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "* The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data.\n",
    "* General documentation classification models not take account of the context.\n",
    "* Generally, deep learning models needs lots of computational power to do the mathematical functions\n",
    "* Users need to have high configuration hardware resources for train a model from scratch to this kind of big size data\n",
    "* So, They proposed an hierarchical attention model based approach for language tasks instead of RNN.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "* The proposed approach combines convolutional layers, Gated Recurrent Units, and attention mechanisms.\n",
    "* It asks which RNN we are going to choose for the model (GRU or LSTM)\n",
    "* It builds the HAHNN architecture using HAN + RNN + CNN sequentially connected and gives the prediction.\n",
    "* It builds with Fast Text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "|Z. Yang [1]|Take into account the contexting importance of words and sentences|IMDB dataset|HAN didn't perform better (accuracy less tan 80%)|\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "\n",
    "* Paper code replication : refer [https://github.com/rubaramanan/ML-Programming-Final-Project/blob/main/hahnn/cnn-hierarchical-network-for-document-classification/Hierarchical%20Attentional%20Hybrid%20Neural%20Networks.pdf](link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "* My contribution code : refer [https://github.com/rubaramanan/ML-Programming-Final-Project/blob/main/src/Movie_Reviews_XLNet.ipynb](link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "For Results each dataset is trained with HAHNN and XLNet model and comparison is present in the form of accuracy, f1-macro and f1-weighted. HAHNN paper only shows accuarcy in their results however I feel f1-score should be a better metric to assess the performance of document classification tasks.\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "* Both the models perform equally well\n",
    "* XLNet model performs well however the execution time was 6+ hours with four epoches, if epoches 8, 13+ hours.\n",
    "\n",
    "|Models|Accuracy|\n",
    "|------|--------|\n",
    "|HAHNN|0.98|\n",
    "|XLNet|0.92|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings : \n",
    "During this project, I learnt how to replicate a paper using github repo and make changes in the source code of a package. I cloned the repo made some changes in the source code for the smooth working of code. I also learnt how we can setup the python environment for Deeplearning projects and I learnt how to customize the LSTM layer using keras library.\n",
    "\n",
    "* HAHNN source code was changed the model from RNN to LSTM to overcome the encountered errors.\n",
    "* Learnt the mechanism of building a text classifier with CNN + RNN + hierachical attention deep learning\n",
    "* Managed to train the XLNet Model with transformers library, which was really easy to code with the help of google colab and transformers library functions.\n",
    "* Same with XLNet model, as it takes 6h+ times to train on a data set with more than 25,000 rows with 4 epochs and 13h+ with 8 epochs\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "Results depicted in the above table summarizes the performance of both the models with same data set. My goal was to see whether fine tuned XLNet model is able to perform better or not compare to HAHNN models.\n",
    "\n",
    "* In the IMDB dataset, Both models perform equally well\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "In terms of Limitations, HAHNN methodology has following problems:\n",
    "\n",
    "* Traditional RNN + CNN training time too long.\n",
    "\n",
    "* Although HAHNN claims to improve the accuracy and robustness of models however they have only worked with pretty standard datasets and did not provide the f1-scores for the document classification datasets and accuracies shown in the paper is achieveable using standalone deep learning architectures or BERT models\n",
    "\n",
    "* They didn't properly mentioned the library version, due to this replication of code was very difficult.\n",
    "\n",
    "In terms of XLNet training the limitations are as follows :\n",
    "\n",
    "* Cannot pad the sentences or input after a certain limit and for longer sentence it is purely loss in information\n",
    "* Hardware limitations also leads to inability in training efficient BERT models (RAM availability)\n",
    "* Training or execution time is endless for more than 10,000 rows.\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "For future considerations, HAHNN can be implemented with new set of embeddings and feature sets such as ELMo and BERT.\n",
    "HAHNN can also be extended to do the extensive hyperparameter tuning by trying every possible parameters and provides the optimal parameters with the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv.org, Oct. 11, 2018. https://arxiv.org/abs/1810.04805 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[2]Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, “XLNet: Generalized Autoregressive Pretraining for Language Understanding,” arXiv.org, Jun. 19, 2019. https://arxiv.org/abs/1906.08237 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[3]R. Johnson and T. Zhang, “Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,” arXiv.org, Feb. 07, 2016. https://arxiv.org/abs/1602.02373 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[4]A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of Tricks for Efficient Text Classification,” arXiv.org, Jul. 06, 2016. https://arxiv.org/abs/1607.01759 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[5]Z. Bingyu and N. Arefyev, “The Document Vectors Using Cosine Similarity Revisited,” arXiv.org, May 26, 2022. https://arxiv.org/abs/2205.13357 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[6]C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to Fine-Tune BERT for Text Classification?,” arXiv.org, May 14, 2019. https://arxiv.org/abs/1905.05583 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[7]J. Abreu, L. Fred, D. Macêdo, and C. Zanchettin, “Hierarchical Attentional Hybrid Neural Networks for Document Classification,” arXiv.org, Jan. 20, 2019. https://arxiv.org/abs/1901.06610 (accessed Aug. 13, 2023).\n",
    "  \n",
    "  \n",
    "[8]Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical Attention Networks for Document Classification,” ACL Anthology. https://aclanthology.org/N16-1174/ (accessed Aug. 13, 2023).\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
