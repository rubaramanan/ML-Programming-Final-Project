{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoekF5mpfCyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9b6ea3de-a59a-4272-d305-3461810548a3"
      },
      "source": [
        "import datetime, pickle, os, codecs, re, string\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import *\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.utils import CustomObjectScope\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "#\n",
        "from keras.engine import InputSpec\n",
        "\n",
        "from keras import initializers\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import string\n",
        "from spacy.lang.en import English\n",
        "import gensim, nltk, logging\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import en_core_web_sm\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "os.environ['PYTHONHASHSEED'] = str(1024)\n",
        "set_random_seed(1024)\n",
        "seed(1024)\n",
        "np.random.seed(1024)\n",
        "random.seed(1024)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOC9MzbzfJ0J",
        "colab_type": "text"
      },
      "source": [
        "# Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWZVSCpQ8q4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = 50\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], 1)))\n",
        "        self.b = K.variable(self.init((self.attention_dim, )))\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
        "        self.trainable_weights = [self.W, self.b, self.u]\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "        ait = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "            \n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_0JG25KfRqq",
        "colab_type": "text"
      },
      "source": [
        "# Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPmVsbH8fQwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HAHNetwork():\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.MAX_SENTENCE_LENGTH = 0\n",
        "        self.MAX_SENTENCE_COUNT = 0\n",
        "        self.VOCABULARY_SIZE = 0\n",
        "        self.word_embedding = None\n",
        "        self.model = None\n",
        "        self.word_attention_model = None\n",
        "        self.tokenizer = None\n",
        "        self.class_count = 2\n",
        "\n",
        "    def build_model(self, n_classes=2, embedding_dim=200, embeddings_path=False):\n",
        "        \n",
        "        l2_reg = regularizers.l2(0.001)\n",
        "        \n",
        "        embedding_weights = np.random.normal(0, 1, (len(self.tokenizer.word_index) + 1, embedding_dim))\n",
        "        \n",
        "        if embeddings_path is not None:\n",
        "\n",
        "            if word_embedding_type is 'from_scratch':\n",
        "                # FastText\n",
        "                filename = './fasttext_model.txt'                \n",
        "                model =  gensim.models.FastText.load(filename)\n",
        "\n",
        "                embeddings_index = model.wv                    \n",
        "                embedding_matrix = np.zeros( ( len(self.tokenizer.word_index) + 1, embedding_dim) )\n",
        "                for word, i in self.tokenizer.word_index.items():\n",
        "                    try:\n",
        "                        embedding_vector = embeddings_index[word]\n",
        "                        if embedding_vector is not None:\n",
        "                            embedding_matrix[i] = embedding_vector\n",
        "                    except Exception as e:\n",
        "                        #print(str(e))\n",
        "                        continue\n",
        "\n",
        "\n",
        "            else:                \n",
        "                embedding_dim = 300\n",
        "                embedding_matrix = load_subword_embedding_300d(self.tokenizer.word_index)\n",
        "\n",
        "            embedding_weights = embedding_matrix\n",
        "\n",
        "        sentence_in = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='int32', name=\"input_1\")\n",
        "        \n",
        "        embedding_trainable = True\n",
        "        \n",
        "        \n",
        "        \n",
        "        if word_embedding_type is 'pre_trained':\n",
        "            embedding_trainable = False\n",
        "        \n",
        "        embedded_word_seq = Embedding(\n",
        "            self.VOCABULARY_SIZE,\n",
        "            embedding_dim,\n",
        "            weights=[embedding_weights],\n",
        "            input_length=self.MAX_SENTENCE_LENGTH,\n",
        "            trainable=embedding_trainable,\n",
        "            #mask_zero=True,\n",
        "            mask_zero=False,\n",
        "            name='word_embeddings',)(sentence_in) \n",
        "        \n",
        "        \n",
        "                     \n",
        "        dropout = Dropout(0.2)(embedded_word_seq)\n",
        "        filter_sizes = [3,4,5]\n",
        "        convs = []\n",
        "        for filter_size in filter_sizes:\n",
        "            conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n",
        "            pool = MaxPool1D(filter_size)(conv)\n",
        "            convs.append(pool)\n",
        "        \n",
        "        concatenate = Concatenate(axis=1)(convs)\n",
        "        \n",
        "        if rnn_type is 'GRU':\n",
        "            #word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True, dropout=0.2))(concatenate)                \n",
        "            dropout = Dropout(0.1)(concatenate)\n",
        "            word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)                \n",
        "        else:\n",
        "            word_encoder = Bidirectional(\n",
        "                LSTM(50, return_sequences=True, dropout=0.2))(embedded_word_seq)\n",
        "            \n",
        "        \n",
        "        dense_transform_word = Dense(\n",
        "            100, \n",
        "            activation='relu', \n",
        "            name='dense_transform_word', \n",
        "            kernel_regularizer=l2_reg)(word_encoder)\n",
        "        \n",
        "        # word attention\n",
        "        attention_weighted_sentence = Model(\n",
        "            sentence_in, Attention(name=\"word_attention\")(dense_transform_word))\n",
        "        \n",
        "        self.word_attention_model = attention_weighted_sentence\n",
        "        \n",
        "        attention_weighted_sentence.summary()\n",
        "\n",
        "        # sentence-attention-weighted document scores\n",
        "        \n",
        "        texts_in = Input(shape=(self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='int32', name=\"input_2\")\n",
        "        \n",
        "        attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n",
        "        \n",
        "        \n",
        "        if rnn_type is 'GRU':\n",
        "            #sentence_encoder = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n",
        "            dropout = Dropout(0.1)(attention_weighted_sentences)\n",
        "            sentence_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)\n",
        "        else:\n",
        "            sentence_encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n",
        "        \n",
        "        \n",
        "        dense_transform_sentence = Dense(\n",
        "            100, \n",
        "            activation='relu', \n",
        "            name='dense_transform_sentence',\n",
        "            kernel_regularizer=l2_reg)(sentence_encoder)\n",
        "        \n",
        "        # sentence attention\n",
        "        attention_weighted_text = Attention(name=\"sentence_attention\")(dense_transform_sentence)\n",
        "        \n",
        "        \n",
        "        prediction = Dense(n_classes, activation='softmax')(attention_weighted_text)\n",
        "        \n",
        "        model = Model(texts_in, prediction)\n",
        "        model.summary()\n",
        "        \n",
        "        \n",
        "        optimizer=Adam(lr=learning_rate, decay=0.0001)\n",
        "\n",
        "        model.compile(\n",
        "                      optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def get_tokenizer_filename(self, saved_model_filename):\n",
        "        return saved_model_filename + '.tokenizer'\n",
        "\n",
        "    def create_reverse_word_index(self):\n",
        "        self.reverse_word_index = {value:key for key,value in self.tokenizer.word_index.items()}\n",
        "\n",
        "    def encode_texts(self, texts):\n",
        "        encoded_texts = np.zeros((len(texts), self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH))\n",
        "        for i, text in enumerate(texts):\n",
        "            encoded_text = np.array(pad_sequences(\n",
        "                self.tokenizer.texts_to_sequences(text), \n",
        "                maxlen=self.MAX_SENTENCE_LENGTH))[:self.MAX_SENTENCE_COUNT]\n",
        "            encoded_texts[i][-len(encoded_text):] = encoded_text\n",
        "        return encoded_texts\n",
        "\n",
        "\n",
        "    def encode_input(self, x, log=False):\n",
        "        x = np.array(x)\n",
        "        if not x.shape:\n",
        "            x = np.expand_dims(x, 0)\n",
        "        texts = np.array([normalize(text) for text in x])\n",
        "        return self.encode_texts(texts)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "            encoded_x = self.encode_texts(x)\n",
        "            return self.model.predict(encoded_x)\n",
        "\n",
        "    \n",
        "    def activation_maps(self, text, websafe=False):\n",
        "        normalized_text = normalize(text)\n",
        "        \n",
        "        encoded_text = self.encode_input(text)[0]\n",
        "\n",
        "        # get word activations\n",
        "        \n",
        "        hidden_word_encoding_out = Model(\n",
        "            inputs=self.word_attention_model.input, \n",
        "            outputs=self.word_attention_model.get_layer('dense_transform_word').output)\n",
        "        \n",
        "        \n",
        "        hidden_word_encodings = hidden_word_encoding_out.predict(encoded_text)\n",
        "        \n",
        "        word_context = self.word_attention_model.get_layer('word_attention').get_weights()[0]\n",
        "\n",
        "        \n",
        "        dot = np.dot(hidden_word_encodings, word_context)\n",
        "        \n",
        "        #u_wattention = encoded_text*np.exp(np.squeeze(dot))\n",
        "        u_wattention = encoded_text\n",
        "        \n",
        "        if websafe:\n",
        "            u_wattention = u_wattention.astype(float)\n",
        "\n",
        "        nopad_encoded_text = encoded_text[-len(normalized_text):]\n",
        "        nopad_encoded_text = [list(filter(lambda x: x > 0, sentence)) for sentence in nopad_encoded_text]\n",
        "        reconstructed_texts = [[self.reverse_word_index[int(i)] \n",
        "                                for i in sentence] for sentence in nopad_encoded_text]\n",
        "        nopad_wattention = u_wattention[-len(normalized_text):]\n",
        "        nopad_wattention = nopad_wattention/np.expand_dims(np.sum(nopad_wattention, -1), -1)\n",
        "        nopad_wattention = np.array([attention_seq[-len(sentence):] \n",
        "                            for attention_seq, sentence in zip(nopad_wattention, nopad_encoded_text)])\n",
        "        word_activation_maps = []\n",
        "        for i, text in enumerate(reconstructed_texts):\n",
        "            word_activation_maps.append(list(zip(text, nopad_wattention[i])))\n",
        "        \n",
        "        hidden_sentence_encoding_out = Model(inputs=self.model.input,\n",
        "                                             outputs=self.model.get_layer('dense_transform_sentence').output)\n",
        "        hidden_sentence_encodings = np.squeeze(\n",
        "            hidden_sentence_encoding_out.predict(np.expand_dims(encoded_text, 0)), 0)\n",
        "        sentence_context = self.model.get_layer('sentence_attention').get_weights()[0]\n",
        "        u_sattention = np.exp(np.squeeze(np.dot(hidden_sentence_encodings, sentence_context), -1))\n",
        "        if websafe:\n",
        "            u_sattention = u_sattention.astype(float)\n",
        "        nopad_sattention = u_sattention[-len(normalized_text):]\n",
        "\n",
        "        nopad_sattention = nopad_sattention/np.expand_dims(np.sum(nopad_sattention, -1), -1)\n",
        "\n",
        "        activation_map = list(zip(word_activation_maps, nopad_sattention))  \n",
        "\n",
        "        return activation_map\n",
        "    \n",
        "    \n",
        "    def load_weights(self, saved_model_dir, saved_model_filename):\n",
        "        with CustomObjectScope({'Attention': Attention}):\n",
        "            print(os.path.join(saved_model_dir, saved_model_filename))\n",
        "            self.model = load_model(os.path.join(saved_model_dir, saved_model_filename))            \n",
        "            self.word_attention_model = self.model.get_layer('time_distributed_1').layer\n",
        "            tokenizer_path = os.path.join(\n",
        "                saved_model_dir, self.get_tokenizer_filename(saved_model_filename))\n",
        "            tokenizer_state = pickle.load(open(tokenizer_path, \"rb\" ))\n",
        "            self.tokenizer = tokenizer_state['tokenizer']\n",
        "            self.MAX_SENTENCE_COUNT = tokenizer_state['maxSentenceCount']\n",
        "            self.MAX_SENTENCE_LENGTH = tokenizer_state['maxSentenceLength']\n",
        "            self.VOCABULARY_SIZE = tokenizer_state['vocabularySize']\n",
        "            self.create_reverse_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkkjbzxsatxp",
        "colab_type": "text"
      },
      "source": [
        "# Normalize texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox0OkPFvasId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '#', '—–']\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    cleanr = re.compile('<.*?>')\n",
        "\n",
        "    # string = re.sub(r'\\d+', '', string)\n",
        "    string = re.sub(cleanr, '', string)\n",
        "    # string = re.sub(\"'\", '', string)\n",
        "    # string = re.sub(r'\\W+', ' ', string)\n",
        "    string = string.replace('_', '')\n",
        "\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "\n",
        "def clean_puncts(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = str(text)    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    text = text.lower().strip()\n",
        "    doc = nlp(text)\n",
        "    filtered_sentences = []\n",
        "    for sentence in doc.sents:                    \n",
        "        sentence = clean_puncts(sentence)\n",
        "        sentence = clean_str(sentence)            \n",
        "        #sentence = remove_stopwords(sentence)                \n",
        "        filtered_sentences.append(sentence)\n",
        "    return filtered_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yevgKDr7eOIl",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns44_Hvh8wh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e0e6e823-a0d2-4c2d-cba0-44cdbc18df3c"
      },
      "source": [
        "model = HAHNetwork()\n",
        "\n",
        "model.load_weights('./saved_models', './model.h5')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "text = \"I absolutely love Daughters of the Night Sky. This is a book that popped up as a free or low-cost Amazon special deal. I don't often succumb to these offers, but the brief description and, to be honest, the cover art intrigued me. I am so glad I did. I would give this book six stars if I could.\"\n",
        "ntext = normalize(text)\n",
        "\n",
        "\n",
        "global graph\n",
        "with graph.as_default():\n",
        "    activation_maps = model.activation_maps(text, websafe=True)\n",
        "    preds = model.predict([ntext])[0]\n",
        "    prediction = np.argmax(preds).astype(float)\n",
        "    data = {'activations': activation_maps, 'normalizedText': ntext, 'prediction': prediction}\n",
        "    print(\"Activations map:\")\n",
        "    print(json.dumps(data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./saved_models/./model.h5\n",
            "Activations map:\n",
            "{\"activations\": [[[[\"i\", 0.0006597176408497163], [\"absolutely\", 0.04908299247921889], [\"love\", 0.014117957514183928], [\"daughters\", 0.41944847605224966], [\"of\", 0.001451378809869376], [\"the\", 0.0003958305845098298], [\"night\", 0.022298456260720412], [\"sky\", 0.4922813036020583], [\".\", 0.0002638870563398865]], 0.21974912946052083], [[[\"this\", 0.0017789072426937739], [\"is\", 0.0011859381617958492], [\"a\", 0.0005929690808979246], [\"book\", 0.08115205421431597], [\"that\", 0.0014400677678949598], [\"popped\", 0.3030072003388395], [\"up\", 0.005167301990681914], [\"as\", 0.0034731046166878443], [\"a\", 0.0005929690808979246], [\"free\", 0.023972892842016095], [\"or\", 0.005336721728081322], [\"low\", 0.07005506141465481], [\"cost\", 0.04862346463362982], [\"amazon\", 0.38881829733163914], [\"special\", 0.0265141889030072], [\"deal\", 0.03811944091486658], [\".\", 0.00016941973739940702]], 0.2073157501646788], [[[\"i\", 9.862321985088169e-05], [\"don\", 0.0017949426012860469], [\"'\", 0.0001577971517614107], [\"t\", 0.0004931160992544084], [\"often\", 0.011716438518284744], [\"succumb\", 0.5820545189159335], [\"to\", 0.00011834786382105803], [\"these\", 0.005049508856365142], [\"offers\", 0.027180559390902994], [\"but\", 0.0004536668113140558], [\"the\", 5.9173931910529015e-05], [\"brief\", 0.09984614777703263], [\"description\", 0.060850526647994], [\"and\", 7.889857588070535e-05], [\"to\", 0.00011834786382105803], [\"be\", 0.0006903625389561718], [\"honest\", 0.015227425144976133], [\"the\", 5.9173931910529015e-05], [\"cover\", 0.0272594579667837], [\"art\", 0.02660854471576788], [\"intrigued\", 0.13933488500532565], [\"me\", 0.0007100871829263482], [\".\", 3.9449287940352674e-05]], 0.19126346748854894], [[[\"i\", 0.005330490405117271], [\"am\", 0.15671641791044777], [\"so\", 0.03411513859275053], [\"glad\", 0.6961620469083155], [\"i\", 0.005330490405117271], [\"did\", 0.10021321961620469], [\".\", 0.0021321961620469083]], 0.1976753082412455], [[[\"i\", 0.0015234613040828763], [\"would\", 0.015234613040828763], [\"give\", 0.05088360755636807], [\"this\", 0.006398537477148081], [\"book\", 0.2918951858622791], [\"six\", 0.5027422303473492], [\"stars\", 0.08226691042047532], [\"if\", 0.013711151736745886], [\"i\", 0.0015234613040828763], [\"could\", 0.033211456429006705], [\".\", 0.0006093845216331506]], 0.18399634464500592]], \"normalizedText\": [\"i absolutely love daughters of the night sky .\", \"this is a book that popped up as a free or low - cost amazon special deal .\", \"i don ' t often succumb to these offers , but the brief description and , to be honest , the cover art intrigued me .\", \"i am so glad i did .\", \"i would give this book six stars if i could .\"], \"prediction\": 4.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuiViYnWc63B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "e3128a74-5615-449f-9309-76c8bfc1058d"
      },
      "source": [
        "display(HTML(\"\"\"<div style=\"height: 400px\"><script async src=\"//jsfiddle.net/luisfredgs/buLaor1x/81/embed/result\"></script></div>\"\"\"), display_id=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"height: 400px\"><script async src=\"//jsfiddle.net/luisfredgs/buLaor1x/81/embed/result\"></script></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DisplayHandle display_id=bb3c7e1db76b2c65356b9c5ac4891ffe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}